{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pipeline with Flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Just for demostration purposes we manually download some data\n",
    "titanic = fetch_openml('Titanic', version=1)\n",
    "data = titanic['data']\n",
    "target = titanic['target']\n",
    "\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.1, shuffle=True, random_state=1234)\n",
    "\n",
    "data_train.to_csv('../data/titanic/titanic.csv', index=False)\n",
    "target_train.to_csv('../data/titanic/titanic_target.csv', index=False)\n",
    "\n",
    "data_test.to_csv('../data/titanic/titanic_test.csv', index=False)\n",
    "target_test.to_csv('../data/titanic/titanic_target_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flux import Flux\n",
    "from flux import CSVDataset, PickleDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Flux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileinput import filename\n",
    "\n",
    "\n",
    "flux = Flux()\n",
    "\n",
    "flux.add_datasets({\n",
    "    \"titanic\":CSVDataset(\n",
    "        filename='../data/titanic/titanic.csv',\n",
    "        save_params={\"index\":False}\n",
    "    ),\n",
    "    \"titanic_target\":CSVDataset(\n",
    "        filename='../data/titanic/titanic_target.csv',\n",
    "        save_params={\"index\":False}\n",
    "    ),\n",
    "    \"titanic_pro\":CSVDataset(\n",
    "        filename='../data/titanic/titanic_pro.csv',\n",
    "        save_params={\"index\":False}\n",
    "    ),\n",
    "    \"imputers\":PickleDataset(\n",
    "        filename=\"../data/titanic/imputers_titanic.pkl\"\n",
    "    ),\n",
    "    \"encoders\":PickleDataset(\n",
    "        filename=\"../data/titanic/encoders_titanic.pkl\"\n",
    "    ),\n",
    "    \"model\":PickleDataset(\n",
    "        filename=\"../data/titanic/model_titanic.pkl\"\n",
    "    ),\n",
    "    \"name_prefixes\":PickleDataset(\n",
    "        filename=\"../data/titanic/name_prefixes.pkl\"\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit - Transform Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "def get_name_prefix(titanic:pd.DataFrame):\n",
    "    titanic['name'] = titanic['name'].apply(lambda s: re.findall(\"[\\w.]+\\.\", s)[0][:-1])\n",
    "    return titanic\n",
    "\n",
    "flux.add_node(\n",
    "    func=get_name_prefix,\n",
    "    inputs='titanic',\n",
    "    outputs='titanic_int',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_common_name_prefix(titanic_int:pd.DataFrame):\n",
    "    # also save the top 10 common prefixes\n",
    "    common_prefixes = titanic_int['name'].value_counts()[:10].index.to_list()\n",
    "    return common_prefixes\n",
    "\n",
    "\n",
    "flux.add_node(\n",
    "    func=save_common_name_prefix,\n",
    "    inputs='titanic_int',\n",
    "    outputs=['name_prefixes'],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_name_prefix_cardinality(titanic_int, name_prefixes):\n",
    "    titanic_int['name'] = titanic_int['name'].apply(lambda x: x if x in name_prefixes else \"other\")\n",
    "    return titanic_int\n",
    "\n",
    "flux.add_node(\n",
    "    func=reduce_name_prefix_cardinality,\n",
    "    inputs=['titanic_int','name_prefixes'],\n",
    "    outputs=['titanic_int_1'],\n",
    ")\n",
    "\n",
    "# flux.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform_data_impute(titanic_int:pd.DataFrame):\n",
    "\n",
    "    num_data = titanic_int.select_dtypes(include='number')\n",
    "    cat_data = titanic_int.select_dtypes(exclude='number')\n",
    "    \n",
    "    print(\"num features\",num_data.columns.to_list())\n",
    "    print(\"cat features\",cat_data.columns.to_list())\n",
    "\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "    imputers = ColumnTransformer([\n",
    "        ('num_imputer', num_imputer, num_data.columns.to_list()),\n",
    "        ('cat_imputer', cat_imputer, cat_data.columns.to_list())\n",
    "    ])\n",
    "\n",
    "    titanic_int_np = imputers.fit_transform(titanic_int)\n",
    "\n",
    "    # return titanic_int_np, imputers\n",
    "\n",
    "    titanic_int = pd.DataFrame(titanic_int_np, columns=imputers.get_feature_names_out(), index=titanic_int.index)\n",
    "\n",
    "    return titanic_int, imputers\n",
    "\n",
    "flux.add_node(\n",
    "    func=fit_transform_data_impute,\n",
    "    inputs='titanic_int_1',\n",
    "    outputs=['titanic_int_2','imputers'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing Existing Node: fit_transform_encode_categorical_features\n",
      "Replacing Existing Node: train_classification_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num features ['pclass', 'age', 'sibsp', 'parch', 'fare', 'body']\n",
      "cat features ['name', 'sex', 'ticket', 'cabin', 'embarked', 'boat', 'home.dest']\n",
      "CV RESULTS\n",
      "fit_time :  0.347\n",
      "score_time :  0.038\n",
      "test_accuracy :  0.952\n",
      "train_accuracy :  0.999\n",
      "test_precision :  0.963\n",
      "train_precision :  0.997\n",
      "test_recall :  0.911\n",
      "train_recall :  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fit_transform_encode_categorical_features(titanic_int:pd.DataFrame):\n",
    "    \n",
    "    cat_data = titanic_int.select_dtypes(exclude='number') \n",
    "    num_data = titanic_int.select_dtypes(include='number')  \n",
    "\n",
    "    ord_features = [f for f in cat_data.columns.to_list() if cat_data[f].nunique()<10]\n",
    "    oh_features = [f for f in cat_data.columns.to_list() if f not in ord_features]\n",
    "\n",
    "    ord_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\",unknown_value=-1)\n",
    "    oh_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore',)\n",
    "\n",
    "    encoders = ColumnTransformer([\n",
    "        ('ordinal_features',ord_encoder, ord_features),\n",
    "        ('one_hot_features',oh_encoder, oh_features)\n",
    "    ])\n",
    "    \n",
    "    cat_data_np = encoders.fit_transform(cat_data)\n",
    "\n",
    "    titanic_out = pd.concat([\n",
    "        pd.DataFrame(cat_data_np, columns=encoders.get_feature_names_out(), index=cat_data.index),\n",
    "        num_data\n",
    "        ],axis=1)\n",
    "    \n",
    "    return titanic_out, encoders\n",
    "\n",
    "\n",
    "flux.add_node(\n",
    "    func=fit_transform_encode_categorical_features,\n",
    "    inputs='titanic_int_2',\n",
    "    outputs=['titanic_int_3','encoders'],\n",
    ")\n",
    "\n",
    "\n",
    "def train_classification_model(titanic_pro:pd.DataFrame, target:pd.Series):\n",
    "\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    results=cross_validate(model, titanic_pro, np.ravel(target), cv=3, return_train_score=True, scoring=['accuracy','precision','recall'])\n",
    "    print(\"CV RESULTS\")\n",
    "    for k,v in results.items():\n",
    "        print(k,\": \",round(np.mean(v),3))\n",
    "\n",
    "    model.fit(titanic_pro, np.ravel(target))\n",
    "\n",
    "    return model\n",
    "\n",
    "flux.add_node(\n",
    "    func=train_classification_model,\n",
    "    inputs=['titanic_int_3','titanic_target'],\n",
    "    outputs=['model'],\n",
    ")\n",
    "\n",
    "\n",
    "flux.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux.save('../data/titanic/fit_flux_titanic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux = Flux()\n",
    "\n",
    "flux.add_datasets({\n",
    "    \"titanic\":CSVDataset(\n",
    "        filename='../data/titanic/titanic_test.csv',\n",
    "        save_params={\"index\":False}\n",
    "    ),\n",
    "    \"titanic_target\":CSVDataset(\n",
    "        filename='../data/titanic/titanic_target_test.csv',\n",
    "        save_params={\"index\":False}\n",
    "    ),\n",
    "    \"imputers\":PickleDataset(\n",
    "        filename=\"../data/titanic/imputers_titanic.pkl\"\n",
    "    ),\n",
    "    \"encoders\":PickleDataset(\n",
    "        filename=\"../data/titanic/encoders_titanic.pkl\"\n",
    "    ),\n",
    "    \"model\":PickleDataset(\n",
    "        filename=\"../data/titanic/model_titanic.pkl\"\n",
    "    ),\n",
    "    \"name_prefixes\":PickleDataset(\n",
    "        filename=\"../data/titanic/name_prefixes.pkl\"\n",
    "    )\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "flux.add_node(\n",
    "    func=get_name_prefix,\n",
    "    inputs='titanic',\n",
    "    outputs='titanic_int',\n",
    ")\n",
    "\n",
    "flux.add_node(\n",
    "    func=reduce_name_prefix_cardinality,\n",
    "    inputs=['titanic_int','name_prefixes'],\n",
    "    outputs=['titanic_int_1'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(titanic_int, imputers ,encoders):\n",
    "\n",
    "    titanic_int_np = imputers.transform(titanic_int)\n",
    "    titanic_int = pd.DataFrame(titanic_int_np, columns=imputers.get_feature_names_out(), index=titanic_int.index)\n",
    "\n",
    "    cat_data = titanic_int.select_dtypes(exclude='number')\n",
    "    num_data = titanic_int.select_dtypes(include='number')\n",
    "    cat_data_np = encoders.transform(cat_data)\n",
    "\n",
    "    titanic_out = pd.concat([\n",
    "        pd.DataFrame(cat_data_np, columns=encoders.get_feature_names_out(), index=cat_data.index),\n",
    "        num_data\n",
    "        ],axis=1)\n",
    "    return titanic_out\n",
    "\n",
    "flux.add_node(\n",
    "    func=transform_data,\n",
    "    inputs=['titanic_int_1','imputers','encoders'],\n",
    "    outputs=['titanic_int_2'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing Existing Node: predictions\n"
     ]
    }
   ],
   "source": [
    "def predictions(titanic_int, model):\n",
    "    return pd.DataFrame(model.predict(titanic_int), index=titanic_int.index, columns=['y_pred'])\n",
    "\n",
    "flux.add_node(\n",
    "    func=predictions,\n",
    "    inputs=['titanic_int_2','model'],\n",
    "    outputs=['y_pred'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "def evaluate_predictions(titanic_target_test, y_pred):\n",
    "    acc=accuracy_score(titanic_target_test, y_pred)\n",
    "    rec=recall_score(titanic_target_test, y_pred)\n",
    "    pre=precision_score(titanic_target_test, y_pred)\n",
    "    return {'acc':acc,\"rec\":rec,\"pre\":pre}\n",
    "\n",
    "flux.add_node(\n",
    "    func=evaluate_predictions,\n",
    "    inputs=['titanic_target','y_pred'],\n",
    "    outputs=['metrics'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "flux()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9465648854961832, 'rec': 0.9375, 'pre': 0.9183673469387755}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flux.load_dataset('metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79ca0e4158c23dff0a706beef246f7bebfcf8a24b6d5d2a00c6cc13b45b3bff0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
