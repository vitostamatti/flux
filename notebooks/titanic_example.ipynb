{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wrangler import Wrangler\n",
    "from wrangler.data import PandasDataset, CSVDataset\n",
    "import wrangler.transformers as tr\n",
    "import wrangler.transformers.text as text_tr\n",
    "import wrangler.transformers.ml as ml_tr\n",
    "from wrangler import logger as wrangler_logger\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Uncomment for console logging \n",
    "wrangler_logger.enable()\n",
    "\n",
    "# Uncomment for file output logging\n",
    "# wrangler_logger.enable_file(filename='logfile.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialize Wrangler and add Datasets\n",
    "\n",
    "To create datasets we can use any of the avaliables in the module ``wrangler.data``. There are datasets for multiple purposes, from in memory data to AWS-S3 data to choose. Each has its own parameters and but all share the same api: \n",
    "\n",
    "- A ``load()`` method which does not take any parameters.\n",
    "- A ``save()``mehtod which takes the new data a parameter.\n",
    "\n",
    "To add a new dataset to the ``Wrangler`` we simply call ``add_dataset()`` and pass the dataset object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:11 | INFO | catalog | Adding dataset: intermediate\n",
      "2022-02-22 at 14:45:11 | INFO | catalog | Adding dataset: titanic\n"
     ]
    }
   ],
   "source": [
    "wrangler = Wrangler()\n",
    "\n",
    "titanic_dataset = CSVDataset(name = 'titanic', filename='../data/titanic_train.csv', save_params={'index':False})\n",
    "\n",
    "wrangler.add_dataset(titanic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pipeline of nodes\n",
    "\n",
    "The main component of the ``Wrangler`` are the ``Nodes`` of transformations. Each one is defined by four parameters.\n",
    "\n",
    "- The ``name``: It's an optional parameter but strongly recommended for better documentation of the process.\n",
    "- The ``transformer``: It's the object in charge of the transformations made in this node. There are a set of transformers already defined in the module ``wrangler.transformers`` and in the submodules:\n",
    "\n",
    "    - ``wrangler.transformers.text``\n",
    "    - ``wrangler.transformers.numeric``\n",
    "    - ``wrangler.transformers.ml``\n",
    "    - ``wrangler.transformers.date``\n",
    "    \n",
    "- The ``inputs``: The name of the dataset (already registered in the wrangler) to read from and pass as input to the transformer\n",
    "- The ``outputs``: The name of the dataset to write the outputs of the transformers. In this case, the dataset does not need to be registered previously.\n",
    "\n",
    "Each transformer can be used and tried outside of the Wrangler by simply calling the ``fit`` and ``transform`` methods.\n",
    "\n",
    "The process of creating a ``Pipeline`` consist on populating the wrangler with nodes by calling the ``add_node`` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:11 | INFO | pipeline | Node drop id y textos added to Pipeline \n",
      "2022-02-22 at 14:45:11 | INFO | pipeline | Node one hot encode sex added to Pipeline \n",
      "2022-02-22 at 14:45:11 | INFO | pipeline | Node fillna numericos added to Pipeline \n",
      "2022-02-22 at 14:45:11 | INFO | pipeline | Node split target added to Pipeline \n",
      "2022-02-22 at 14:45:11 | INFO | pipeline | Node evaluacion added to Pipeline \n",
      "2022-02-22 at 14:45:11 | INFO | pipeline | Node prediccion added to Pipeline \n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrangler.add_node(\n",
    "    name='drop id y textos',\n",
    "    transformer=tr.ColumnDropper(columns=['PassengerId','Name','Ticket','Cabin','Embarked']),\n",
    "    inputs='titanic',\n",
    "    outputs='titanic_pro',\n",
    ")\n",
    "\n",
    "wrangler.add_node(\n",
    "    name='one hot encode sex',\n",
    "    transformer=text_tr.OneHotEncoderTransformer(column='Sex'),\n",
    "    inputs='titanic_pro',\n",
    "    outputs='titanic_pro',\n",
    ")\n",
    "\n",
    "def fillna_numerics(df):\n",
    "    numericos = ['int64', 'float64','float16', 'int16', 'int32', 'float16', 'float32']\n",
    "    for col in df.columns:\n",
    "        if(df[col].dtypes in numericos):\n",
    "            df[col]=df[col].fillna(0)\n",
    "    return df\n",
    "\n",
    "wrangler.add_node(\n",
    "    name='fillna numericos',\n",
    "    transformer=tr.DataframeTransformer(fillna_numerics),\n",
    "    inputs='titanic_pro',\n",
    "    outputs='titanic_pro',\n",
    ")\n",
    "\n",
    "\n",
    "wrangler.add_node(\n",
    "    name='split target',\n",
    "    transformer=ml_tr.SplitFeaturesTarget(target='Survived'),\n",
    "    inputs='titanic_pro',\n",
    "    outputs=['titanic_X','titanic_y'],\n",
    ")\n",
    "\n",
    "\n",
    "wrangler.add_node(\n",
    "    name='evaluacion',\n",
    "    transformer=ml_tr.ClassificationModelEvaluator(\n",
    "        model=RandomForestClassifier(), \n",
    "        ),\n",
    "    inputs=['titanic_X','titanic_y'],\n",
    "    outputs=['model_report'],\n",
    ")\n",
    "\n",
    "wrangler.add_node(\n",
    "    name='prediccion',\n",
    "    transformer=ml_tr.SklearnModelTransformer(model=RandomForestClassifier(),\n",
    "                                              name='rf_clf',\n",
    "                                              filename= '../data/titanic_rf_clf'),\n",
    "    inputs=['titanic_X','titanic_y'],\n",
    "    outputs=['titanic_pred']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Wrangler\n",
    "\n",
    "To excecute the Wrangler we have two options:\n",
    "\n",
    "- Calling ``fit_transform`` method: It calls the ``fit`` and the ``transform`` method of all te nodes in the wrangler.\n",
    "- Calling ``transform`` method: It calls only the ``transform`` method of the nodes.\n",
    "\n",
    "This is very usefull to separate the training process from the testing or production process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:11 | INFO | node | Running Node: drop id y textos\n",
      "2022-02-22 at 14:45:11 | INFO | catalog | Loading dataset: titanic\n",
      "2022-02-22 at 14:45:11 | DEBUG | base | Loading CSVDataset(name='titanic', filename='../data/titanic_train.csv', save_params=dict)\n",
      "2022-02-22 at 14:45:11 | DEBUG | base | Fitting ColumnDropper(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'])\n",
      "2022-02-22 at 14:45:11 | DEBUG | base | Transforming ColumnDropper(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'])\n",
      "2022-02-22 at 14:45:11 | INFO | catalog | Saving dataset: titanic_pro\n",
      "2022-02-22 at 14:45:11 | INFO | catalog | Adding dataset: titanic_pro\n",
      "2022-02-22 at 14:45:11 | INFO | node | Running Node: one hot encode sex\n",
      "2022-02-22 at 14:45:11 | INFO | catalog | Loading dataset: titanic_pro\n",
      "2022-02-22 at 14:45:11 | DEBUG | base | Loading PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:11 | DEBUG | base | Fitting OneHotEncoderTransformer(column='Sex')\n",
      "/home/tomas/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "2022-02-22 at 14:45:11 | DEBUG | base | Transforming OneHotEncoderTransformer(column='Sex', encoder=OneHotEncoder, one_hot_columns=ndarray)\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Saving dataset: titanic_pro\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Saving PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:12 | INFO | node | Running Node: fillna numericos\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Loading dataset: titanic_pro\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Loading PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Fitting DataframeTransformer(function=fillna_numerics)\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Transforming DataframeTransformer(function=fillna_numerics)\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Saving dataset: titanic_pro\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Saving PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:12 | INFO | node | Running Node: split target\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Loading dataset: titanic_pro\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Loading PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Fitting SplitFeaturesTarget(target='Survived')\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Transforming SplitFeaturesTarget(target='Survived')\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Saving dataset: titanic_X\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Adding dataset: titanic_X\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Saving dataset: titanic_y\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Adding dataset: titanic_y\n",
      "2022-02-22 at 14:45:12 | INFO | node | Running Node: evaluacion\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Loading dataset: titanic_X\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Loading PandasDataset(name='titanic_X', data=DataFrame)\n",
      "2022-02-22 at 14:45:12 | INFO | catalog | Loading dataset: titanic_y\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Loading PandasDataset(name='titanic_y', data=DataFrame)\n",
      "2022-02-22 at 14:45:12 | DEBUG | base | Fitting ClassificationModelEvaluator(name='ClassificationModelEvaluator', model=RandomForestClassifier, metrics=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'])\n",
      "2022-02-22 at 14:45:13 | DEBUG | base | Transforming ClassificationModelEvaluator(name='ClassificationModelEvaluator', model=RandomForestClassifier, metrics=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], results=DataFrame)\n",
      "2022-02-22 at 14:45:13 | INFO | catalog | Saving dataset: model_report\n",
      "2022-02-22 at 14:45:13 | INFO | catalog | Adding dataset: model_report\n",
      "2022-02-22 at 14:45:13 | INFO | node | Running Node: prediccion\n",
      "2022-02-22 at 14:45:13 | INFO | catalog | Loading dataset: titanic_X\n",
      "2022-02-22 at 14:45:13 | DEBUG | base | Loading PandasDataset(name='titanic_X', data=DataFrame)\n",
      "2022-02-22 at 14:45:13 | INFO | catalog | Loading dataset: titanic_y\n",
      "2022-02-22 at 14:45:13 | DEBUG | base | Loading PandasDataset(name='titanic_y', data=DataFrame)\n",
      "2022-02-22 at 14:45:13 | DEBUG | base | Fitting SklearnModelTransformer(name='rf_clf', model=RandomForestClassifier, filename='../data/titanic_rf_clf')\n",
      "2022-02-22 at 14:45:14 | DEBUG | base | Transforming SklearnModelTransformer(name='rf_clf', model=RandomForestClassifier, filename='../data/titanic_rf_clf')\n",
      "2022-02-22 at 14:45:14 | INFO | catalog | Saving dataset: titanic_pred\n",
      "2022-02-22 at 14:45:14 | INFO | catalog | Adding dataset: titanic_pred\n"
     ]
    }
   ],
   "source": [
    "wrangler.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Results\n",
    "\n",
    "To ``Wrangler`` saves all its inputs and outputs in a data catalog. We can retrieve that data by calling the ``data_catalog.load`` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intermediate': PandasDataset(name='intermediate', data=DataFrame), 'titanic': CSVDataset(name='titanic', filename='../data/titanic_train.csv', save_params=dict), 'titanic_pro': PandasDataset(name='titanic_pro', data=DataFrame), 'titanic_X': PandasDataset(name='titanic_X', data=DataFrame), 'titanic_y': PandasDataset(name='titanic_y', data=DataFrame), 'model_report': PandasDataset(name='model_report', data=DataFrame), 'titanic_pred': PandasDataset(name='titanic_pred', data=DataFrame)}\n"
     ]
    }
   ],
   "source": [
    "print(wrangler.data_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:14 | INFO | catalog | Loading dataset: model_report\n",
      "2022-02-22 at 14:45:14 | DEBUG | base | Loading PandasDataset(name='model_report', data=DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.496035</td>\n",
       "      <td>0.095066</td>\n",
       "      <td>0.759777</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.846904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.268788</td>\n",
       "      <td>0.047642</td>\n",
       "      <td>0.820225</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>0.720588</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.812968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246877</td>\n",
       "      <td>0.050850</td>\n",
       "      <td>0.876404</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.901270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252931</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>0.775281</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>0.847928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.239624</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>0.831461</td>\n",
       "      <td>0.774648</td>\n",
       "      <td>0.797101</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.896822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  test_precision  test_recall   test_f1  \\\n",
       "0  0.496035    0.095066       0.759777        0.685714     0.695652  0.690647   \n",
       "1  0.268788    0.047642       0.820225        0.790323     0.720588  0.753846   \n",
       "2  0.246877    0.050850       0.876404        0.870968     0.794118  0.830769   \n",
       "3  0.252931    0.053372       0.775281        0.750000     0.617647  0.677419   \n",
       "4  0.239624    0.045305       0.831461        0.774648     0.797101  0.785714   \n",
       "\n",
       "   test_roc_auc  \n",
       "0      0.846904  \n",
       "1      0.812968  \n",
       "2      0.901270  \n",
       "3      0.847928  \n",
       "4      0.896822  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangler.data_catalog.load(\"model_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Wrangler\n",
    "\n",
    "To save and pickle the fitted wrangler, we simply call ``save`` method with the path and the name of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangler.save('../data/titanic_wrangler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Fitted Wrangler\n",
    "\n",
    "To load a fitted wrangler there are two steps:\n",
    "\n",
    "- Create a new empty wrangler.\n",
    "- Call ``load`` with the path to the fitted wrangler.\n",
    "\n",
    "In order to apply transformations to new data, we need to add it to the data_catalog, otherwise the wrangler will mantain the reference to the fitted data.\n",
    "\n",
    "For this task we have two options:\n",
    "\n",
    "- Creating a new dataset with the new reference and calling ``add_dataset``.\n",
    "- Calling the ``datasets_from_config`` method to load the new references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:15 | INFO | catalog | Adding dataset: intermediate\n"
     ]
    }
   ],
   "source": [
    "loaded_wrangler = Wrangler()\n",
    "loaded_wrangler.load('../data/titanic_wrangler')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding datasets with ``add_dataset``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:15 | INFO | catalog | Adding dataset: titanic\n"
     ]
    }
   ],
   "source": [
    "titanic_dataset = CSVDataset(name = 'titanic', filename='../data/titanic_test.csv')\n",
    "\n",
    "loaded_wrangler.add_dataset(titanic_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding datasets with ``datasets_from_config``\n",
    "\n",
    "- We call the ``datasets_to_config`` of the fitted wrangler to generate a .yml file with the current reference of the datasets. This step can be done before saving it.\n",
    "- Then we can modify this .yml and pass it to ``datasets_from_config``.\n",
    "\n",
    "\n",
    "Example of .yml file:\n",
    "\n",
    "    inputs:\n",
    "        titanic:\n",
    "            filename: ../data/titanic_train.csv\n",
    "            type: CSVDataset\n",
    "    outputs:\n",
    "        model_report:\n",
    "            data: DataFrame\n",
    "            type: PandasDataset\n",
    "        titanic_pred:\n",
    "            data: DataFrame\n",
    "            type: PandasDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wrangler.wrangler.Wrangler at 0x7f53e9594df0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrangler.datasets_to_config(\"../data/titanic_datasets.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: intermediate\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: titanic\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: model_report\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: titanic_pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<wrangler.wrangler.Wrangler at 0x7f539d947a00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_wrangler_from_config = Wrangler()\n",
    "loaded_wrangler_from_config.load('../data/titanic_wrangler')\n",
    "loaded_wrangler_from_config.datasets_from_config(\"../data/titanic_datasets_new.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform new Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:16 | INFO | node | Running Node: drop id y textos\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading CSVDataset(name='titanic', filename='../data/titanic_test.csv')\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Transforming ColumnDropper(columns=['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'])\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | INFO | node | Running Node: one hot encode sex\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Transforming OneHotEncoderTransformer(column='Sex', encoder=OneHotEncoder, one_hot_columns=ndarray)\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Saving PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | INFO | node | Running Node: fillna numericos\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Transforming DataframeTransformer(function=fillna_numerics)\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Saving PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | INFO | node | Running Node: split target\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_pro\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_pro', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Transforming SplitFeaturesTarget(target='Survived')\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: titanic_X\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: titanic_X\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: titanic_y\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: titanic_y\n",
      "2022-02-22 at 14:45:16 | INFO | node | Running Node: evaluacion\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_X\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_X', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_y\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_y', data=Series)\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Transforming ClassificationModelEvaluator(name='ClassificationModelEvaluator', model=RandomForestClassifier, metrics=['accuracy', 'precision', 'recall', 'f1', 'roc_auc'], results=DataFrame)\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: model_report\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: model_report\n",
      "2022-02-22 at 14:45:16 | INFO | node | Running Node: prediccion\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_X\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_X', data=DataFrame)\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Loading dataset: titanic_y\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Loading PandasDataset(name='titanic_y', data=Series)\n",
      "2022-02-22 at 14:45:16 | DEBUG | base | Transforming SklearnModelTransformer(name='rf_clf', model=RandomForestClassifier, filename='../data/titanic_rf_clf')\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Saving dataset: titanic_pred\n",
      "2022-02-22 at 14:45:16 | INFO | catalog | Adding dataset: titanic_pred\n"
     ]
    }
   ],
   "source": [
    "loaded_wrangler.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 at 14:45:17 | INFO | catalog | Loading dataset: titanic_pred\n",
      "2022-02-22 at 14:45:17 | DEBUG | base | Loading PandasDataset(name='titanic_pred', data=DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rf_clf_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rf_clf_prediction\n",
       "0                    0\n",
       "1                    0\n",
       "2                    1\n",
       "3                    1\n",
       "4                    0\n",
       "..                 ...\n",
       "413                  0\n",
       "414                  1\n",
       "415                  0\n",
       "416                  0\n",
       "417                  1\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_wrangler.data_catalog.load('titanic_pred')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "``wrangler`` has some extra features:\n",
    "\n",
    "- The ``plot_wrangler``: it generates a graph plot of the wrangler with its inputs, outputs and nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8279/1864595237.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwrangler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_wrangler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplot_wrangler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaded_wrangler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/Tomas/Documents/1. Clientes/wrangler/wrangler/extras/wrangler_viz.py\u001b[0m in \u001b[0;36mplot_wrangler\u001b[0;34m(wrangler, output_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_wrangler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrangler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mWrangler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnohtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "from wrangler.extras import plot_wrangler\n",
    "\n",
    "plot_wrangler(loaded_wrangler)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "477c64429a66b88dbea596d2fd7dadb0d3a75c4333ee684689ddf8e8bb5bf8be"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
